# Machine setups
## EC2
VM Details:
type:           t2.micro
Virtual CPU:    1 (base: ??? GHz, turbo: 3.3 GHz according to [5])
RAM:            3.75GB
Virtualization: Hardware Virtual Machine (HVM) ~ hardware-assisted virtualization

## GCloud
VM Details:
type:           n1-standard-1
Virtual CPU:    1 (base: 2 GHz according to [3], turbo: 3.5 GHz)
RAM:            3.75GB
Virtualization: Kernel-based Virtual Machine (KVM) -> based on https://en.wikipedia.org/wiki/Google_Compute_Engine
                KVM ! hardware-assisted virtualization

# Measuring Units
CPU: KFLOPS
MEM: seconds
Sequential: bytes/second
Random: megabytes/second

# CPU benchmark results
## Look at linpack.sh and linpack.c and shortly describe how the benchmark works.

LINPACK is a benchmark for measuring the CPU performance.
Firstly, the linpack.c gets compiled for the benchmark by the shell script linpack.sh.
The program is separated into three different parts which all do extensive floating point arithmetics.
Each of them solves a linear equation system under different conditions like the matrix size or the required precision of the results.
After the compilation linpack.sh executes the C-program and filters the output for the required metric.

## Find out what the LINPACK benchmark measures (try Google). Would you expect paravirtualization to affect the LINPACK benchmark? Why?

The performance is represented by FLOPS (floating point operations per second) metric.
The benchmark only executes calculations on the CPU.
Since it does not involve critical (privileged) instructions, we would not expect the paravirtualization to affect the LINPACK benchmark.

## Look at your LINPACK measurements. Are they consistent with your expectations? If not, what could be the reason?

Both machines are running on one virtual core.
However, since we do not know what kind of CPU is backing this virtual core the results could still be very different.
In general we would assume that both machines have nearly the same performance since both of them belong to a low-cost instance and both of them have a similar turbo clock frequency.

Although we were not quite sure about the base frequency of the EC2 t2.micro instance, we expected that both machines have a similar performance.
The results show that the GCloud performes in average 272,167.7604 KFLOPS more than the EC2 instance which is about 12.4%.
The average KFLOPS for EC2 is 2,194,698.108 KFLOPS.
The average KFLOPS for GCloud is 2,466,865.868 KFLOPS.

# Memory benchmark results
## Find out how the memsweep benchmark works by looking at the shell script and the C code. Would you expect virtualization to affect the memsweep benchmark? Why?

The memsweep benchmark measures how fast main memory can be accessed and overwritten.
Therefore, a huge heap array with a size of 33,161,216 (~33MB) bytes gets created.
In the next step different locations of the heap array are accessed and overwritten.

Different kinds of virtualization will have a different effect on the memsweep benchmark.
### Full virtualization
The memsweep benchmark does not create more pages than we have available in the VM (since 33MB fit into 1GB RAM).
As long as there are no other VM's running on the same host, that get assigned for some computing cycles, there should be no chance that a page fault occurs.
Additionally, full virtualiztion introduces one additional step for memory look ups caused by the added level of indirection.

### OS-assisted
In os-assisted virtualization each guest system maintains it's own page tables. The hypervisor supervises their actions and makes sure that their write accesses have sufficient privileges.
It makes use of command batching for the allocation and therefore the hypervisor would only have to check this (batched) system call ones.

### hardware-assisted
Hardware-assisted VM's require much less interventions because they use a tagged Transaction-Lookaside-Buffer (TLB).
This also makes it unneccessary to hold any shadow page tables resulting in less memory overhead.

However, since both of the VM's are running in a hardware-assisted virtualization environment and both machines have sufficient memory capacities to store the 33MB, we do not expect big differences.

## Look at your memsweep measurements. Are they consistent with your expectations? If not, what could be the reason?

Our measurements show, that the average run time of the benchmark algorithm for AWS is 5.89 seconds and for GCloud is 6.31 seconds.
GCloud is therefore 7% faster than AWS.
To conclude you can say, that both perform similar and that the results meet our expectations.

# Disk benchmark questions

## Look at the disk measurements. Are they consistent with your expectations? If not, what could be the reason? + Compare the results for the two operations (sequential, random). What are reasons for the differences


We would expect that the reads are generally faster than writes and that sequential read/writes are faster than random read/writes.
As we do not know what kind of drive is used (SSD or HDD) we assume that both use HDD since it is cheaper.
The results of both, EC2 and GCloud, are expected to be similar.

Our measurements show that the read benchmarks perform similar on both setups.
GCloud performs way better when it comes to the write operations.

On the EC2 setup the sequential read is much slower than the random read (by ~90%). On the GCloud setup the random read is 85% faster than the sequential read.
This is not as expected.
While double checking our scripts we did not find any flaws, besides that we are flushing the cache for the sequential read, but are not doing this for the random read.
This could have a big impact on our results for the sequential read operations, leading to way lower numbers than we would expect.
This should explain the differences we mentioned above.

Comparing the random read/write operations, we see the same speed for both for EC2.
On GCloud we see similar effects, however the random read performance is not very stable and varies a lot.

The sequential write performs 16% faster than the sequential read on EC2 machine.
On GCloud the write is 47% faster than the read.

While the sequential results can be explained with the caching issue, the results for the random operations remain obscure.

To conclude, none of our expectations were met.

Other guest systems might have influenced our benchmark results if they were running on the same host machines as our VM's.
